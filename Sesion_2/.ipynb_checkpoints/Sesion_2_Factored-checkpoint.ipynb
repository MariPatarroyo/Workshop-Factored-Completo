{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a58918",
   "metadata": {},
   "source": [
    "# Pipelines en scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c61b0b3",
   "metadata": {},
   "source": [
    "El objetivo de esta sección es mostrar cómo se pueden desarrollar flujos de preprocesamiento en sklearn. Vamos a explorar las diferentes operaciones que se le pueden hacer a los diferentes tipos de variables y cómo agrupar todas las operaciones en un solo elemento de sklearn que tenga los métodos `.fit()` y `.transform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae00b53f",
   "metadata": {},
   "source": [
    "Es importante siempre separar la variable dependiente—en este caso `trip_duration` del dataframe que vamos a usar para crear las variables independientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb58727",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"https://factored-workshops.s3.amazonaws.com/taxi-trip-duration.csv\"\n",
    ")\n",
    "# Limitar rango de datos\n",
    "tiempo_minimo = 60 # 1 minuto\n",
    "tiempo_maximo = 36000 # 10 horas\n",
    "data = data[\n",
    "    (data[\"trip_duration\"] > tiempo_minimo) &\n",
    "    (data[\"trip_duration\"] < tiempo_maximo)\n",
    "]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"store_and_fwd_flag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccfb743",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"trip_duration\"]\n",
    "input_df = data.drop(\n",
    "    [\"id\", \"trip_duration\", \"dropoff_datetime\", \"store_and_fwd_flag\"],\n",
    "    axis=\"columns\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dc569f",
   "metadata": {},
   "source": [
    "## División de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43440161",
   "metadata": {},
   "source": [
    "Un paso que siempre se debe hacer es dividir los datos disponibles entrenamiento, validación y test. En este caso vamos a dividir los datos disponibles en sets de entrenamiento y validación con la función`train_test_split` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662f20d",
   "metadata": {},
   "source": [
    "$$NOTA$$ Suponemos que train_df realmente es train _x, igual en la validación. Cuando no se indica el número de datos para hacer la validación, el train_test_split reconocerá por defecto el 25% de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efd6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df, y_train, y_val = train_test_split(input_df, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90f7de",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fcaab9",
   "metadata": {},
   "source": [
    "scikit-learn incluye una gran [lista](https://scikit-learn.org/stable/data_transforms.html) de transformers que nos permiten limpiar y transformar datos dependiendo de nuestro objetivo. Sin importar el tipo de transformacion que apliquen, todos los transformers siguen la convención de tener por lo menos dos métodos:\n",
    "\n",
    "   * `.fit()` calcula los parámetros necesarios para realizar la transformación a partir de unos datos de entrada. Este método se ejecuta únicamente en los datos de entrenamiento para asegurarnos que los parámetros no contienen información de los datos de validación.\n",
    "    \n",
    "   * `.transform()` aplica la transformación a los datos.\n",
    "\n",
    "Veamos un ejemplo usando`StandardScaler`, un transformer que nos permite remover la media y escalar los datos para que tengan varianza de 1. Vamos a usarlo para normalizar los coordenadas de inicio del viaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "transformer = StandardScaler()\n",
    "transformer.fit(\n",
    "    train_df[[\"pickup_longitude\", \"pickup_latitude\"]]\n",
    ")\n",
    "normed_array = transformer.transform(\n",
    "    val_df[[\"pickup_longitude\", \"pickup_latitude\"]]\n",
    ")\n",
    "print(normed_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac35d8",
   "metadata": {},
   "source": [
    "## Custom Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c6ca3",
   "metadata": {},
   "source": [
    "A pesar de que `scikit-learn` ofrece varias operaciones para transformar datos, frecuentemente necesitamos crear transformaciones que son específicas a nuestro proyecto. Para esto vamos a aprender cómo implementar custom transformers.\n",
    "\n",
    "Todos los transformer custom deben heredar `BaseEstimator` y `TransformerMixin` para que tengan todas las funciones necesarias para conectarse a otros objetos de sklearn. Por convención de sklearn, los objetos usados para transformar datos siempre deben tener los métodos `.fit()` y `.transform()`. Ambos métodos reciben X y y para que se integren sin problemas a pipelines de sklearn.\n",
    "\n",
    "El método `.fit()` sirve para almacenar cantidades que vamos a usar durante la transformación de los datos y siempre debe retornar `self`. El método`.transform()` ejecuta la transformación y retorna los datos transformados. Ahora vamos a replicar el `StandardScaler` pero esta vez escribiéndolo como un transformer personalizado que no retorne un array sino un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class PrimerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.mean = X.mean()\n",
    "        self.std = X.std()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return (X - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbdc3cc",
   "metadata": {},
   "source": [
    "## Checkpoint 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edb428",
   "metadata": {},
   "source": [
    "Correr `.fit()` y `.transform()` para `PrimerTransformer` para las coordenadas de inicio del viaje y verificar que el resultado sea igual al del `StandardScaler`. \n",
    "\n",
    "De `PrimerTransformer` va a salir un `DataFrame` en lugar de un array pero los valores deben ser los mismos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c358cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "primer_transformer = PrimerTransformer()\n",
    "primer_transformer.fit(train_df[[\"pickup_longitude\", \"pickup_latitude\"]])\n",
    "val_normed_df = primer_transformer.transform(val_df[[\"pickup_longitude\", \"pickup_latitude\"]])\n",
    "print(val_normed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b60ef",
   "metadata": {},
   "source": [
    "En este [link](https://scikit-learn.org/stable/developers/develop.html) pueden encontrar más detalles de qué papel juegan los métodos __init__, .fit() y .transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ffba70",
   "metadata": {},
   "source": [
    "# Transformer Fechas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078aa600",
   "metadata": {},
   "source": [
    "Ahora que sabemos cómo construir objetos para transformar datos, vamos a crear un transformer para crear variables como el día de la semana y la hora del momento en el que empieza el servicio. Como vimos la semana pasada en nuestro EDA, esa información puede ser relevante para determinar la duración del viaje.\n",
    "\n",
    "En nuestra transformación no debemos almacenar ningún dato para hacer las transformación entonces dejamos el método `.fit()` vacío. En `.transform()` extraemos los datos de fecha que nos interesan y retornamos un dataframe con las nuevas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc93cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFechas(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        columna_fecha = pd.to_datetime(X[\"pickup_datetime\"])\n",
    "        fecha_df = pd.DataFrame()\n",
    "        fecha_df[\"weekday\"] = columna_fecha.dt.weekday\n",
    "        fecha_df[\"hour\"] = columna_fecha.dt.hour\n",
    "        # TODO: Crear columnas con dia de la semana y hora de recogida.\n",
    "        return fecha_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864d1ee4",
   "metadata": {},
   "source": [
    "## Checkpoint 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c29985",
   "metadata": {},
   "source": [
    "Completar el codigo para extrar el día de la semana y la hora a partir de una fecha en pandas y los ponga en las columnas weekday y hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5467b3d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer_fechas = TransformerFechas()\n",
    "fechas_df = transformer_fechas.fit_transform(train_df)\n",
    "fechas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da49c1",
   "metadata": {},
   "source": [
    "## Transformer Distancia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a12c3d",
   "metadata": {},
   "source": [
    "También queremos crear una feature que nos ayude a medir la distancia entre el punto de origen y el punto de destino usando la longitud y la latitud en los datos. Nuevamente no tenemos que almacenar cantidades en nuestro método `.fit()` y calculamos la distancia entre los dos puntos usando la [distancia de Haversine](https://es.wikipedia.org/wiki/F%C3%B3rmula_del_semiverseno). En el código ya está implementada la función para calcular la distancia y no es necesario fijarse en los detalles de la implementación. Como pueden ver, nuestros transformers pueden incluir funciones adicionales que nos ayuden a calcular cantidades útiles para realizar la transformación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58488c",
   "metadata": {},
   "source": [
    "## Check point 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a37fb6",
   "metadata": {},
   "source": [
    "Calcular la distancia de Haversine usando el método que está implementado en la clase y almacenarlo en la variable distancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f74df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDistancia(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_init = X[[\"pickup_latitude\", \"pickup_longitude\"]].to_numpy()\n",
    "        X_final = X[[\"dropoff_latitude\", \"dropoff_longitude\"]].to_numpy()\n",
    "\n",
    "        # Distancia de Haversine\n",
    "        # TODO: Calcular la variable distancia usando la funcion\n",
    "        # distancia de Haversine.\n",
    "        distancia = self.distancia_haversine(X_init = X_init, X_final = X_final)\n",
    "        distancia_df = pd.DataFrame()\n",
    "        distancia_df[\"distancia\"] = distancia\n",
    "        return distancia_df\n",
    "    \n",
    "    def distancia_haversine(self, X_init, X_final):\n",
    "        # Convertir de decimal a radianes\n",
    "        X_init = np.radians(X_init)\n",
    "        X_final = np.radians(X_final)\n",
    "\n",
    "        # Formula Haversine\n",
    "        dlat = X_final[:, 0] - X_init[:, 0] \n",
    "        dlon = X_final[:, 1] - X_init[:, 1]\n",
    "        a = np.sin(dlat / 2) ** 2 + np.cos(X_init[:, 0]) * np.cos(X_final[:, 0]) * np.sin(dlon / 2) ** 2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "        return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ba40f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer_dist = TransformerDistancia()\n",
    "distancias_df = transformer_dist.fit_transform(train_df)\n",
    "distancias_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a82e0e",
   "metadata": {},
   "source": [
    "# Unión de transformer con Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b98ace",
   "metadata": {},
   "source": [
    "## Pipeline Numérico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aaebc5",
   "metadata": {},
   "source": [
    "Ahora vamos a usar los objetos de `Pipeline` y `ColumnTransformer` para unir los transformers que ya hemos creado con otros disponibles en `sklearn` y lograr todas las transformaciones que queremos realizar a todos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a69be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ccbaea",
   "metadata": {},
   "source": [
    "`ColumnTransformer` nos permite elegir las columnas sobre las que queremos aplicar una transformación cuando nos llegan columnas adicionales. En este caso queremos que al TransformerDistancia llegue únicamente pickup_longitude, pickup_latitude, dropoff_longitude y dropoff_latitude`ColumnTransformer` nos permite elegir las columnas sobre las que queremos aplicar una transformación cuando nos llegan columnas adicionales. En este caso queremos que al TransformerDistancia llegue únicamente `pickup_longitude`, `pickup_latitude`, `dropoff_longitud` y `dropoff_latitude `.\n",
    "Leyendo la [documentación](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer) sabemos que debemos pasar una tupla con el nombre del transformer, la clase que define el transformer y las columnas sobre las que queremos aplicar la transformación. ColumnTransformer también nos permite definir qué se debe hacer con las columnas que no estamos transformando; en este caso elegimos pasarlas sin transformarlas `remainder=\"passthrough\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_cols = [\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\"\n",
    "]\n",
    "\n",
    "transformer_coord = ColumnTransformer(\n",
    "    [\n",
    "        (\"transformer_dist\", TransformerDistancia(), coord_cols),\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "display(transformer_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf3934",
   "metadata": {},
   "source": [
    "Ahora que tenemos nuestro transformer que realiza transformación usando las coordenadas y deja pasar columnas adicionales, vamos a usarlo para unirlo con la otra variable númerica que tenemos en el dataset—`passenger_count`- y normalizarlas usando `StandardScalr`. Para esto vamos a usar el objeto Pipeline.\n",
    "`Pipeline`[documentación Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) nos permite concatenar transformaciones de `sklearn`. En este caso vamos a concatenar el `ColumnTransformer`que creamos con `TransformerDistancia`con el `StandardScaler`. Como `passenger_count`no estaba entre columnas que selecciona transformer_coord, esa columna pasa directamente a ser normalizada por el`StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e15edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"passenger_count\"] + coord_cols\n",
    "\n",
    "num_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"transformer_coord\", transformer_coord),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_num = num_pipeline.fit_transform(train_df[num_cols], y_train)\n",
    "print(X_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449847cc",
   "metadata": {},
   "source": [
    "Usando la función`display` podemos ver el diagrama del flujo que acabamos de crear. Por `TransformerDistancia` pasan las variables de longitud y latitud y por passthrough pasa cualquier columna que le pasemos al `ColumnTransformer` que no sean longitud o latitud—en este caso`passenger_count`. El último paso es aplicar un`StandardScaler` a la variable de distancia que sale de `TransformerDistancia` y a las variables que vienen de`passthrough`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06032b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(num_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d9ee9",
   "metadata": {},
   "source": [
    "## Pipeline Categórico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41747cb",
   "metadata": {},
   "source": [
    "Usaremos una lógica similar para las variables categóricas pero esta vez para concatenar el resultado de extraer variables temporales de la fecha de inicio del viaje con un`OneHotEncoder` para las variables categóricas de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad36df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "cat_cols = [\"vendor_id\", \"pickup_borough\", \"pickup_datetime\"]\n",
    "\n",
    "transformer_fechas = ColumnTransformer(\n",
    "    [\n",
    "        #TODO: Punto 1 de Checkpoint 3\n",
    "        (\"transfomer_fechas\", TransformerFechas(), [\"pickup_datetime\"])\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "cat_pipeline = Pipeline(\n",
    "    [\n",
    "        #TODO: Punto 2 de Checkpoint 3\n",
    "        (\"transformer_fechas\", transformer_fechas),\n",
    "        (\"ordinal_encoder\", OrdinalEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_cat = cat_pipeline.fit_transform(train_df[cat_cols])\n",
    "print(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e20965",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cat_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c501fcdb",
   "metadata": {},
   "source": [
    "## Checkpoint 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e841c8b",
   "metadata": {},
   "source": [
    "1. Crear un ColumnTransformer llamado transformer_fechas que solo seleccione la variable pickup_datetime para aplicarle el TransformerFechas. \n",
    "2. Unir el ColumnTransformer del punto 1 con un OrdinalEncoder para transformar las variables que salen del ColumnTransformer de fechas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8830f2",
   "metadata": {},
   "source": [
    "# Unión de Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b027a08e",
   "metadata": {},
   "source": [
    "Por último, usaremos nuevamente el`ColumnTransformer` para determinar cuáles son las variables numéricas y las variables catégoricas en nuestros datos. Definiendo varios elementos en la lista que le pasamos al`ColumnTransformer` le hacemos saber a`sklearn` que queremos diferentes transformaciones para las columnas y al final queremos unirlas para que todas queden en un solo`numpy` array. En nuestro caso, unimos las columnas que resultan del preprocesamiento de las categóricas y del de las numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "full_pipeline = ColumnTransformer(\n",
    "    [\n",
    "        (\"num_pipeline\", num_pipeline, num_cols),\n",
    "        (\"cat_pipeline\", cat_pipeline, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_transformed = full_pipeline.fit_transform(train_df, y_train)\n",
    "print(X_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e3e61",
   "metadata": {},
   "source": [
    "En este diagrama podemos ver que ejecutamos las transformaciones por separado para las variables numéricas y las categóricas y al final las unimos para tener los datos en un array que le pasaremos al modelo al momento de entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054150d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(full_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae2c05",
   "metadata": {},
   "source": [
    "## Guardar Pipeline a Disco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024aa10",
   "metadata": {},
   "source": [
    "Además de tener una clara separación entre los métodos`.fit()` y `.transform()`, la ventaja de escribir todo nuestro proceso como un solo pipeline (`full_pipeline`) es que podemos guardar en disco el objeto que usamos para preprocesar. Esto es útil para tener todas las transformaciones definidas en un objeto al momento que queramos hacer reproducibles nuestras transformaciones para, por ejemplo, desplegar nuestro modelo.\n",
    "\n",
    "Creamos un contexto usand `open()` y usamos la función `dill.dump()` para guardar nuestro flujo de preprocesamiento en disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45329c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.settings['recurse'] = True\n",
    "\n",
    "with open(<TODO: Nombre del archivo>, \"wb\") as f:\n",
    "    dill.dump(full_pipeline, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439f9d6",
   "metadata": {},
   "source": [
    "Para cargar nuestro flujo, usamos la función `dill.load()`. Además verificamos que el resultado de las transformaciones con el flujo que teníamos en el notebook y el que cargamos desde el disco es idéntico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(<TODO: Nombre del archivo>, \"rb\") as f:\n",
    "    loaded_pipeline = dill.load(f)\n",
    "    \n",
    "X_loaded = loaded_pipeline.transform(train_df)\n",
    "print((X_loaded == X_transformed).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d0c084",
   "metadata": {},
   "source": [
    "El bloque anterior debería imprimir `True` ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8397853",
   "metadata": {},
   "source": [
    "## Checkpoint 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e5721d",
   "metadata": {},
   "source": [
    "Guardar pipeline de preprocesamiento en un archivo y volver a cargarlo con éxito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2cae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.settings['recurse'] = True\n",
    "\n",
    "with open(\"preprocesser.pkl\", \"wb\") as f:\n",
    "    dill.dump(full_pipeline, f, protocol=dill.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"preprocesser.pkl\", \"rb\") as f:\n",
    "    loaded_pipeline = dill.load(f)\n",
    "    \n",
    "X_loaded = loaded_pipeline.transform(train_df)\n",
    "print((X_loaded == X_transformed).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b7c7e3",
   "metadata": {},
   "source": [
    "# Scikit-learn y MLflow\n",
    "En esta sección veremos como todos los modelos de`sklearn` tiene la misma API para entrenar todo tipo de modelos (en este caso de regresión). Veremos cómo usar los métodos`.fit()` y `.predict()`. Además veremos cómo usar MLflow para guardar métricas y parámetros de todos los modelos que corramos. Si estás haciendo esta sección en el mismo notebook de la sección anterior, puedes saltar hasta Transformación de Datos.\n",
    "Empezaremos cargando los datos y el preprocesador que construimos en la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75498109",
   "metadata": {},
   "source": [
    "En este bloque de código vamos a copiar los transformers de la sección anterior. Normalmente los importaríamos de un archivo .py por los dejaremos en el notebook por simplicidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFechas(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        columna_fecha = pd.to_datetime(X[\"pickup_datetime\"])\n",
    "        fecha_df = pd.DataFrame()\n",
    "        fecha_df[\"weekday\"] = columna_fecha.dt.weekday\n",
    "        fecha_df[\"hour\"] = columna_fecha.dt.hour\n",
    "        return fecha_df\n",
    "\n",
    "class TransformerDistancia(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_init = X[[\"pickup_latitude\", \"pickup_longitude\"]].to_numpy()\n",
    "        X_final = X[[\"dropoff_latitude\", \"dropoff_longitude\"]].to_numpy()\n",
    "\n",
    "        # Distancia de Haversine\n",
    "        distancia = self.distancia_haversine(X_init=X_init, X_final=X_final)\n",
    "        distancia_df = pd.DataFrame()\n",
    "        distancia_df[\"distancia\"] = distancia\n",
    "        return distancia_df\n",
    "    \n",
    "    def distancia_haversine(self, X_init, X_final):\n",
    "        # Convertir de decimal a radianes\n",
    "        X_init = np.radians(X_init)\n",
    "        X_final = np.radians(X_final)\n",
    "\n",
    "        # haversine formula \n",
    "        dlat = X_final[:, 0] - X_init[:, 0] \n",
    "        dlon = X_final[:, 1] - X_init[:, 1]\n",
    "        a = np.sin(dlat / 2) ** 2 + np.cos(X_init[:, 0]) * np.cos(X_final[:, 0]) * np.sin(dlon / 2) ** 2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "        return c * r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b019e3e",
   "metadata": {},
   "source": [
    "Aquí cargamos los datos y el preprocesador entrenado de la sección anterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://factored-workshops.s3.amazonaws.com/taxi-trip-duration.csv\")\n",
    "# Limitar rango de datos\n",
    "tiempo_minimo = 60 # 1 minuto\n",
    "tiempo_maximo = 36000 # 10 horas\n",
    "data = data[\n",
    "    (data[\"trip_duration\"] > tiempo_minimo) &\n",
    "    (data[\"trip_duration\"] < tiempo_maximo)\n",
    "]\n",
    "\n",
    "y = data[\"trip_duration\"]\n",
    "selected_columns = [\n",
    "    'vendor_id',\n",
    "    'pickup_datetime',\n",
    "    'passenger_count',\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'pickup_borough',\n",
    "    'dropoff_borough'\n",
    "]\n",
    "input_df = data[selected_columns]\n",
    "train_df, val_df, y_train, y_val = train_test_split(input_df, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2a9780",
   "metadata": {},
   "source": [
    "Ya no es necesario hacer `.fit()` para aplicar las transformaciones al dataframe de entrada porque toda la información quedó almacenada en el archivo `preprocesser.pkl` y se carga al usar `dill.load()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0fc2d",
   "metadata": {},
   "source": [
    "## Transformación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cec5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"preprocesser.pkl\", \"rb\") as f:\n",
    "    preprocessor = dill.load(f)\n",
    "\n",
    "X_train = preprocessor.transform(train_df)\n",
    "X_val = preprocessor.transform(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95c42d",
   "metadata": {},
   "source": [
    "## Entrenar Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf4159",
   "metadata": {},
   "source": [
    "### Baseline con `DummyRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f3644",
   "metadata": {},
   "source": [
    "Una opción que nos ofrece `sklearn` es entrenar modelos dummy. Estos son modelos que predicen lo mismo para todos los casos (en este caso el promedio) y nos sirven como una medida de cuál es el rendimiento mínimo para un problema. Si por alguna razón nuestro modelo tiene peor desempeño que el modelo dummy, tenemos que revisar nuestro modelo porque probablemente tenemos errores en la implementación.\n",
    "En este caso vamos a usar el `DummyRegressor` que siempre predice la media de los datos de entrenamiento. La función evaluar_predicciones es una ayuda para poder calcular varias métricas de un modelo usando metricas incluídas de `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score, mean_squared_log_error\n",
    "\n",
    "def evaluar_predicciones(y_pred, y_true):\n",
    "    mae = mean_absolute_error(y_pred=y_pred, y_true=y_true)\n",
    "    mape = mean_absolute_percentage_error(y_pred=y_pred, y_true=y_true)\n",
    "    rmse = mean_squared_error(y_pred=y_pred, y_true=y_true, squared=False)\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MAPE: {mape}\")\n",
    "    print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dummy_model = DummyRegressor(strategy=\"mean\")\n",
    "dummy_model.fit(X_train, y_train)\n",
    "y_train_dummy = dummy_model.predict(X_train)\n",
    "y_val_dummy = dummy_model.predict(X_val)\n",
    "\n",
    "print(\"TRAIN\")\n",
    "evaluar_predicciones(y_pred=y_train_dummy, y_true=y_train)\n",
    "\n",
    "print(\"VALIDATION\")\n",
    "evaluar_predicciones(y_pred=y_val_dummy, y_true=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44344c",
   "metadata": {},
   "source": [
    "### Baseline con regresión lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2badc561",
   "metadata": {},
   "source": [
    "Otra buena medida es siempre empezar con un modelo lineal. Así tenemos una medida de cómo se desempeña un modelo sencillo.\n",
    "Algo para notar es que sin importar el modelo, los modelos de`sklearn` siempre siguen el mismo proceso. \n",
    "1. Inicializar modelo con la clase de `sklearn`. \n",
    "2. Ejecutar función `.fit(X_train, y_train`. \n",
    "3. Ejecutar función `.predict(X)` para generar las predicciones.\n",
    "Esta consistencia en el proceso simplifica el uso de diferentes modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f14367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#TODO: entrenar un modelo lineal con LinearRegression\n",
    "\n",
    "print(\"TRAIN\")\n",
    "evaluar_predicciones(y_pred=y_train_linear, y_true=y_train)\n",
    "\n",
    "print(\"VALIDATION\")\n",
    "evaluar_predicciones(y_pred=y_val_linear, y_true=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d967e7",
   "metadata": {},
   "source": [
    "### Checkpoint 1\n",
    "Entrenar un model de regresión linear usando LinearRergression de scikit-learn.\n",
    "\n",
    "Predecir valores del viaje para train “(`y_train_linear`) y validación (`y_val_linear`)\n",
    "\n",
    "Las métricas del modelo deberían ser similares a las imagen de arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_train_linear = linear_model.predict(X_train)\n",
    "y_val_linear = linear_model.predict(X_val)\n",
    "\n",
    "print(\"TRAIN\")\n",
    "evaluar_predicciones(y_pred=y_train_linear, y_true=y_train)\n",
    "\n",
    "print(\"VALIDATION\")\n",
    "evaluar_predicciones(y_pred=y_val_linear, y_true=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751dccfb",
   "metadata": {},
   "source": [
    "## ¿Cómo comparar modelos más allá de un print? MLFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378093f9",
   "metadata": {},
   "source": [
    "Imprimiendo los resultados nos podemos dar cuenta del modelo con mejores resultados. Sin embargo, comparar los resultados usando print es difícil cuando el número de modelos empieza a crecer. Además idealmente queremos que todos los modelos queden guardados para poder retomarlos después. Para todo esto podemos usar MLflow.\n",
    "\n",
    "[MLflow](https://www.mlflow.org/) es una librería open-source que nos ayuda a manejar todo el ciclo de modelos de machine learning. En este caso vamos a ver cómo podemos usar MLflow para guardar resultados del desempeño de modelos para determinar cuál modelo es el mejor.\n",
    "Vamos a correr los mismos modelos que acabamos de comparar pero esta vez vamos a usar MLflow para ver los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81055db6",
   "metadata": {},
   "source": [
    "`mlflow.sklearn.autolog()` automáticamente nos ayuda a guardar varias métricas y parámetros del modelo. Sin embargo, no incluye valores para datos de validación. Para guardar métricas que no se incluyen en `.autolog` vamos a usar la función `mlflow.log_metric` dentro del context manager de `run`.\n",
    "\n",
    "Vamos primero a usar el modelo dummy y vamos a poner `run_name=\"dummy` para poder verlo con el nombre correcto en MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"dummy\") as run:\n",
    "    dummy_model.fit(X_train, y_train)\n",
    "    y_pred_val = dummy_model.predict(X_val)\n",
    "    val_mae = mean_absolute_error(y_pred=y_pred_val, y_true=y_val)\n",
    "    val_rmse = mean_squared_error(y_pred=y_pred_val, y_true=y_val, squared=False)\n",
    "    val_mape = mean_absolute_percentage_error(y_pred=y_pred_val, y_true=y_val)\n",
    "    val_r2 = r2_score(y_pred=y_pred_val, y_true=y_val)\n",
    "\n",
    "    mlflow.log_metric(\"val_mae\", val_mae)\n",
    "    mlflow.log_metric(\"val_rmse\", val_rmse)\n",
    "    mlflow.log_metric(\"val_mape\", val_mape)\n",
    "    mlflow.log_metric(\"val_r2\", val_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3399a91",
   "metadata": {},
   "source": [
    "Una vez termine de correr esta celda debemos abrir la interfaz de MLflow para ver los resultados. Deberíamos encontrar una nueva carpeta llamada `mlruns` en la ubicación donde corrió este notebook. Una vez estemos en la carpeta donde está `mlruns`, debemos ir a un terminal a esa ubicación y ejecutamos el comando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260fc7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ff727",
   "metadata": {},
   "source": [
    "Ahí nos debería salir el mensaje de que está iniciando un servidor y dar la ruta para acceder. (normalmente la ruta es [http://127.0.0.1:5000](). Copiamos esa dirección en un navegador y ahí ya deberíamos ver la interfaz de MLflow.\n",
    "\n",
    "En la interfaz de MLflow estarán los modelos que corramos usando MLflow y podemos filtrar u ordenar por métricas o parámetros que consideramos importantes. En general, con la interfaz de MLflow tenemos una base de datos de modelos que podemos analizar fácilmente.\n",
    "Además, si hacemos click un alguna de las runs (click en la columna Start Time) podemos ver que tenemos los datos más detallados y los artifacts de cada run. Los artifacts pueden ser todo tipo de archivos (por ejemplo imágenes y CSVs) que nos ayudan a guardar lo relevante de cada caso como el modelo o el ambiente de conda desde el que corrimos el modelo.\n",
    "\n",
    "Ahora corramos nuevamente la regresión lineal para que quede registrada en MLflow. En este caso vamos a guardar nuestro preprocesador que quede asociado a cada run usando .log_artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a1568",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"linear_regression\") as run:\n",
    "    #TODO: Hacer fit a un modelo lineal\n",
    "    y_pred_val = linear_model.predict(X_val)\n",
    "    val_mae = mean_absolute_error(y_pred=y_pred_val, y_true=y_val)\n",
    "    val_rmse = mean_squared_error(y_pred=y_pred_val, y_true=y_val, squared=False)\n",
    "    val_mape = mean_absolute_percentage_error(y_pred=y_pred_val, y_true=y_val)\n",
    "    val_r2 = r2_score(y_pred=y_pred_val, y_true=y_val)\n",
    "\n",
    "    mlflow.log_metric(\"val_mae\", val_mae)\n",
    "    mlflow.log_metric(\"val_rmse\", val_rmse)\n",
    "    mlflow.log_metric(\"val_mape\", val_mape)\n",
    "    mlflow.log_metric(\"val_r2\", val_r2)\n",
    "    mlflow.log_artifact(\"preprocesser.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ef78f",
   "metadata": {},
   "source": [
    "Ahora que tenemos la regresión lineal y el modelo dummy en MLflow, vamos a probar con un modelo más complejo como el Random Forest. Por ahora vamos a dejar los parámetros por defecto que trae la clase `RandomForestRegressor` excepto `n_jobs` que lo vamos a poner con el parámetro `n_jobs=2`. Poner `n_jobs=2` hace que el modelo corra en paralelo en 2 procesadores.\n",
    "Entrenar el random forest puede tomar algunos minutos así que es un buen momento para resolver cualquier duda o tomar un descanso y estirar las piernas :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a278d",
   "metadata": {},
   "source": [
    "## Checkpoint 2\n",
    "Entrenar un modelo lineal y un random forest y verificar las métricas de validación en MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"linear_regression\") as run:\n",
    "    linear_model.fit(X_train, y_train)\n",
    "    y_pred_val = linear_model.predict(X_val)\n",
    "    log_metrics_mlflow(y_pred_val, y_val)\n",
    "    mlflow.log_artifact(\"preprocesser.pkl\")\n",
    "    \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "with mlflow.start_run(run_name=\"random_forest\") as run:\n",
    "    rf_model = RandomForestRegressor(n_jobs=2)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred_val = rf_model.predict(X_val)\n",
    "    log_metrics_mlflow(y_pred_val, y_val)\n",
    "    mlflow.log_artifact(\"preprocesser.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50096f27",
   "metadata": {},
   "source": [
    "Con esas herramientas puedes buscar más [modelos de regresión de sklearn](https://scikit-learn.org/stable/supervised_learning.html) o mejorar los hiperparámetros de los modelos que usamos para intentar mejorar las métricas que obtuvimos hasta el momento. Incluso librerías como [XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn) o [LightGBM](https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api) ofrecen la opción de crear modelos con la API de sklearn. A pesar de que cada modelo puede usar técnicas muy diferentes, el código va a ser casi idéntico gracias a la consistencia en la API de scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
